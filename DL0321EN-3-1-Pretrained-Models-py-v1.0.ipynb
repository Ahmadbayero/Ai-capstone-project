{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/Logos/organization_logo/organization_logo.png\" width = 400> </a>\n",
    "\n",
    "<h1 align=center><font size = 5>Pre-Trained Models</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3> \n",
    "\n",
    "1.  <a href=\"https://#item31\">Import Libraries and Packages</a>\n",
    "2.  <a href=\"https://#item32\">Download Data</a>\n",
    "3.  <a href=\"https://#item33\">Define Global Constants</a>\n",
    "4.  <a href=\"https://#item34\">Construct ImageDataGenerator Instances</a>\n",
    "5.  <a href=\"https://#item35\">Compile and Fit Model</a>\n",
    "\n",
    "</font>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item31'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Import Libraries and Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Let's start the lab by importing the libraries that we will be using in this lab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n",
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-18 17:58:58.478997: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-07-18 17:58:58.479128: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2022-07-18 17:58:58.480203: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-07-18 17:58:58.480219: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "\n",
    "if tensorflow.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "   print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item32'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Download Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "For your convenience, I have placed the data on a server which you can retrieve easily using the **wget** command. So let's run the following line of code to get the data. Given the large size of the image dataset, it might take some time depending on your internet speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n"
     ]
    }
   ],
   "source": [
    "## get the data\n",
    "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download(url, filename):\n",
    "    \n",
    "    response = requests.get(url, stream=True)\n",
    "    \n",
    "    with open(filename, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=(512)):\n",
    "            if chunk:\n",
    "                file.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip'\n",
    "# filename = 'concrete_data_week3.zip'\n",
    "\n",
    "# download(url, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "And now if you check the left directory pane, you should see the zipped file *concrete_data_week3.zip* appear. So, let's go ahead and unzip the file to access the images. Given the large number of images in the dataset, this might take a couple of minutes, so please be patient, and wait until the code finishes running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !unzip concrete_data_week3.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50**\\* error. So please **DO NOT DO IT**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item33'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Define Global Constants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Here, we will define constants that we will be using throughout the rest of the lab.\n",
    "\n",
    "1.  We are obviously dealing with two classes, so *num_classes* is 2.\n",
    "2.  The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n",
    "3.  We will training and validating the model using batches of 100 images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "image_resize = 224\n",
    "\n",
    "batch_size_training = 100\n",
    "batch_size_validation = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item34'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Construct ImageDataGenerator Instances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next, we will use the *flow_from_directory* method to get the training images as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/train',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## Type your answer here\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Double-click **here** for the solution.\n",
    "\n",
    "<!-- The correct answer is:\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item35'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Build, Compile and Fit Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this section, we will start building our model. We will use the Sequential model class from Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-18 17:59:00.184044: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-07-18 17:59:00.184077: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.add(ResNet50(\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    weights='imagenet',\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "You can access the model's layers using the *layers* attribute of our model object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.functional.Functional at 0x29bc34eb0>,\n",
       " <keras.layers.core.dense.Dense at 0x29bc11bb0>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "You can access the ResNet50 layers by running the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x13f9ff1c0>,\n",
       " <keras.layers.reshaping.zero_padding2d.ZeroPadding2D at 0x13f9ff580>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x13f9c8400>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x14faab670>,\n",
       " <keras.layers.core.activation.Activation at 0x14fa47400>,\n",
       " <keras.layers.reshaping.zero_padding2d.ZeroPadding2D at 0x14fe18f40>,\n",
       " <keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x14fe9f160>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x14fe9f8e0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x14faab880>,\n",
       " <keras.layers.core.activation.Activation at 0x14fe96e80>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x14feae400>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x14fecb8b0>,\n",
       " <keras.layers.core.activation.Activation at 0x14feae5b0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x14fe9f7c0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x14fed8970>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x13f9c8e80>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x14fed2f40>,\n",
       " <keras.layers.merging.add.Add at 0x14fed2100>,\n",
       " <keras.layers.core.activation.Activation at 0x14fee29d0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x14fee7fa0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x14feedeb0>,\n",
       " <keras.layers.core.activation.Activation at 0x14fed8070>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x14fefab50>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x162708ac0>,\n",
       " <keras.layers.core.activation.Activation at 0x1627087f0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x1627211c0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x162721670>,\n",
       " <keras.layers.merging.add.Add at 0x14fefa430>,\n",
       " <keras.layers.core.activation.Activation at 0x162725eb0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x162721c70>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x16273b6d0>,\n",
       " <keras.layers.core.activation.Activation at 0x16273b820>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x14fefa670>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x14fed8e50>,\n",
       " <keras.layers.core.activation.Activation at 0x162737640>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x14fe8f9a0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x16272e0a0>,\n",
       " <keras.layers.merging.add.Add at 0x14fefa5e0>,\n",
       " <keras.layers.core.activation.Activation at 0x14feae790>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x162765eb0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x162774730>,\n",
       " <keras.layers.core.activation.Activation at 0x14fee7bb0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x162787160>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x162787070>,\n",
       " <keras.layers.core.activation.Activation at 0x1627919a0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x162765a90>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x16279ceb0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x162765970>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1631a64c0>,\n",
       " <keras.layers.merging.add.Add at 0x1631a6a90>,\n",
       " <keras.layers.core.activation.Activation at 0x16279c790>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x1631b9f10>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1631af7c0>,\n",
       " <keras.layers.core.activation.Activation at 0x1631c19a0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x1631cbbe0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1631b67c0>,\n",
       " <keras.layers.core.activation.Activation at 0x1631b9d90>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x162791a90>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x14fe9fa60>,\n",
       " <keras.layers.merging.add.Add at 0x14fe9f640>,\n",
       " <keras.layers.core.activation.Activation at 0x1631d1130>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x1631d1190>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1627376a0>,\n",
       " <keras.layers.core.activation.Activation at 0x14fef5580>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x1627a16a0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1631dfa60>,\n",
       " <keras.layers.core.activation.Activation at 0x162737fa0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x16ce16a30>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x16ce2fb20>,\n",
       " <keras.layers.merging.add.Add at 0x16ce16fa0>,\n",
       " <keras.layers.core.activation.Activation at 0x16ce23bb0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x16ce38d90>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x16ce2f3d0>,\n",
       " <keras.layers.core.activation.Activation at 0x16ce38640>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x16ce4beb0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x16ce40c40>,\n",
       " <keras.layers.core.activation.Activation at 0x16cf72940>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x16cf722b0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x16cf6dbe0>,\n",
       " <keras.layers.merging.add.Add at 0x16ce453d0>,\n",
       " <keras.layers.core.activation.Activation at 0x16ce2fd00>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x16ce27280>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x162767be0>,\n",
       " <keras.layers.core.activation.Activation at 0x1631b9970>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x1631a6cd0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x16cf82490>,\n",
       " <keras.layers.core.activation.Activation at 0x16cf7fa90>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x16cf67cd0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x16cf8f6d0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1631df610>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x16cf95bb0>,\n",
       " <keras.layers.merging.add.Add at 0x16cf9dd60>,\n",
       " <keras.layers.core.activation.Activation at 0x176de9490>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x176de9f40>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x176dd9e50>,\n",
       " <keras.layers.core.activation.Activation at 0x176de9a90>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x176df9d90>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x176df9ca0>,\n",
       " <keras.layers.core.activation.Activation at 0x176df97f0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x176e0cca0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x176e296d0>,\n",
       " <keras.layers.merging.add.Add at 0x176e1bfa0>,\n",
       " <keras.layers.core.activation.Activation at 0x176e11a60>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x176e0cd90>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x176e1b7c0>,\n",
       " <keras.layers.core.activation.Activation at 0x176e0c460>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x162767f40>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x176df9730>,\n",
       " <keras.layers.core.activation.Activation at 0x16cf954c0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x16cf7fe20>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x16cf7b5b0>,\n",
       " <keras.layers.merging.add.Add at 0x16cf67f70>,\n",
       " <keras.layers.core.activation.Activation at 0x176e30190>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x176e3e610>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x176e3efa0>,\n",
       " <keras.layers.core.activation.Activation at 0x176e39f10>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x176e4ce20>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x176e3ec70>,\n",
       " <keras.layers.core.activation.Activation at 0x176e54700>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x176e5fb80>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x176e6baf0>,\n",
       " <keras.layers.merging.add.Add at 0x176e6b820>,\n",
       " <keras.layers.core.activation.Activation at 0x176e5f3a0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x176e83cd0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x176e83a30>,\n",
       " <keras.layers.core.activation.Activation at 0x176e90160>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x176e90c10>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x176e6b2b0>,\n",
       " <keras.layers.core.activation.Activation at 0x176e769a0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x176e54cd0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x176e33ee0>,\n",
       " <keras.layers.merging.add.Add at 0x176e33190>,\n",
       " <keras.layers.core.activation.Activation at 0x176e110d0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x176e30b20>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x176e9c5e0>,\n",
       " <keras.layers.core.activation.Activation at 0x176e9aeb0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x176ea2b80>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x176ea86d0>,\n",
       " <keras.layers.core.activation.Activation at 0x176e9c850>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x176eb2b80>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x176ebeaf0>,\n",
       " <keras.layers.merging.add.Add at 0x176ebe820>,\n",
       " <keras.layers.core.activation.Activation at 0x176eb2370>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x176ec7f10>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x177fc1ee0>,\n",
       " <keras.layers.core.activation.Activation at 0x176de9e20>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x177fdd400>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x177fddd90>,\n",
       " <keras.layers.core.activation.Activation at 0x177fdd3a0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x176ed7a30>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x176ed7820>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x176eced00>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x176eb2d30>,\n",
       " <keras.layers.merging.add.Add at 0x1631d7b50>,\n",
       " <keras.layers.core.activation.Activation at 0x176eaaf70>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x16ce277f0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x16ce38280>,\n",
       " <keras.layers.core.activation.Activation at 0x176eaa190>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x176e332b0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x176ea80a0>,\n",
       " <keras.layers.core.activation.Activation at 0x176e9a3a0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x177fe8b80>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x176e54be0>,\n",
       " <keras.layers.merging.add.Add at 0x1041a3f10>,\n",
       " <keras.layers.core.activation.Activation at 0x176e33bb0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x177feeac0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x177fee1c0>,\n",
       " <keras.layers.core.activation.Activation at 0x29bc05fd0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x29bc29310>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x29bc19550>,\n",
       " <keras.layers.core.activation.Activation at 0x29bc1ff70>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x29bc29490>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2bfb104f0>,\n",
       " <keras.layers.merging.add.Add at 0x2bfb01bb0>,\n",
       " <keras.layers.core.activation.Activation at 0x2bfb01a00>,\n",
       " <keras.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D at 0x16cf7b400>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].layers\n",
    "\n",
    "# plot_model(ResNet50(\n",
    "#     include_top=False,\n",
    "#     pooling='avg',\n",
    "#     weights='imagenet',\n",
    "#     ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 2048)              23587712  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 4098      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next we compile our model using the **adam** optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "steps_per_epoch_training = len(train_generator)\n",
    "steps_per_epoch_validation = len(validation_generator)\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-18 17:59:02.036034: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-07-18 17:59:03.714801: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301/301 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9942"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-18 18:01:47.928025: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301/301 [==============================] - 220s 720ms/step - loss: 0.0194 - accuracy: 0.9942 - val_loss: 0.0056 - val_accuracy: 0.9986\n",
      "Epoch 2/2\n",
      "301/301 [==============================] - 218s 726ms/step - loss: 0.0046 - accuracy: 0.9989 - val_loss: 0.0043 - val_accuracy: 0.9988\n"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=steps_per_epoch_validation,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now that the model is trained, you are ready to start using it to classify images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.save('classifier_resnet_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by Alex Aklson. I hope you found this lab interesting and educational.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3\\_LAB1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n",
    "\n",
    "| Date (YYYY-MM-DD) | Version | Changed By | Change Description                                          |\n",
    "| ----------------- | ------- | ---------- | ----------------------------------------------------------- |\n",
    "| 2020-09-18        | 2.0     | Shubham    | Migrated Lab to Markdown and added to course repo in GitLab |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr>\n",
    "\n",
    "Copyright Â© 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_medium=dswb&utm_source=bducopyrightlink&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
